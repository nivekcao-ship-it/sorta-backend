/**
 *  Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
 *
 *  Licensed under the Apache License, Version 2.0 (the "License"). You may not use this file except in compliance
 *  with the License. A copy of the License is located at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 *  or in the 'license' file accompanying this file. This file is distributed on an 'AS IS' BASIS, WITHOUT WARRANTIES
 *  OR CONDITIONS OF ANY KIND, express or implied. See the License for the specific language governing permissions
 *  and limitations under the License.
 */
import { CfnAgent } from 'aws-cdk-lib/aws-bedrock';
import { IFunction } from 'aws-cdk-lib/aws-lambda';
import { IInvokable } from '../models';
/**
 * The step in the agent sequence that this prompt configuration applies to.
 */
export declare enum AgentStepType {
    PRE_PROCESSING = "PRE_PROCESSING",
    ORCHESTRATION = "ORCHESTRATION",
    POST_PROCESSING = "POST_PROCESSING",
    ROUTING_CLASSIFIER = "ROUTING_CLASSIFIER",
    MEMORY_SUMMARIZATION = "MEMORY_SUMMARIZATION",
    KNOWLEDGE_BASE_RESPONSE_GENERATION = "KNOWLEDGE_BASE_RESPONSE_GENERATION"
}
/**
 * LLM inference configuration
 */
export interface InferenceConfiguration {
    /**
     * The likelihood of the model selecting higher-probability options while
     * generating a response. A lower value makes the model more likely to choose
     * higher-probability options, while a higher value makes the model more
     * likely to choose lower-probability options.
     *
     * Floating point
     *
     * min 0
     * max 1
     */
    readonly temperature: number;
    /**
     * While generating a response, the model determines the probability of the
     * following token at each point of generation. The value that you set for
     * Top P determines the number of most-likely candidates from which the model
     * chooses the next token in the sequence. For example, if you set topP to
     * 80, the model only selects the next token from the top 80% of the
     * probability distribution of next tokens.
     *
     * Floating point
     *
     * min 0
     * max 1
     */
    readonly topP: number;
    /**
     * While generating a response, the model determines the probability of the
     * following token at each point of generation. The value that you set for
     * topK is the number of most-likely candidates from which the model chooses
     * the next token in the sequence. For example, if you set topK to 50, the
     * model selects the next token from among the top 50 most likely choices.
     *
     * Integer
     *
     * min 0
     * max 500
     */
    readonly topK: number;
    /**
     * A list of stop sequences. A stop sequence is a sequence of characters that
     * causes the model to stop generating the response.
     *
     * length 0-4
     */
    readonly stopSequences: string[];
    /**
     * The maximum number of tokens to generate in the response.
     *
     * Integer
     *
     * min 0
     * max 4096
     */
    readonly maximumLength: number;
}
/**
 * Contains configurations to override a prompt template in one part of an agent sequence.
 */
export interface PromptStepConfiguration {
    /**
     * The step in the agent sequence where to set a specific prompt configuration.
     */
    readonly stepType: AgentStepType;
    /**
     * Whether to enable or skip this step in the agent sequence.
     * @default - The default state for each step type is as follows.
     *
     *     PRE_PROCESSING – ENABLED
     *     ORCHESTRATION – ENABLED
     *     KNOWLEDGE_BASE_RESPONSE_GENERATION – ENABLED
     *     POST_PROCESSING – DISABLED
     */
    readonly stepEnabled?: boolean;
    /**
     * The custom prompt template to be used.
     *
     * @default - The default prompt template will be used.
     * @see https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-placeholders.html
     */
    readonly customPromptTemplate?: string;
    /**
     * The inference configuration parameters to use.
     */
    readonly inferenceConfig?: InferenceConfiguration;
    /**
     * The foundation model to use for this specific prompt step.
     * This allows using different models for different steps in the agent sequence.
     *
     * @default - The agent's default foundation model will be used.
     */
    readonly foundationModel?: IInvokable;
}
export interface PromptStepConfigurationCustomParser extends PromptStepConfiguration {
    /**
     * Whether to use the custom Lambda parser defined for the sequence.
     *
     * @default - false
     */
    readonly useCustomParser?: boolean;
}
export interface CustomParserProps {
    readonly parser?: IFunction;
    readonly steps?: PromptStepConfigurationCustomParser[];
}
export declare class PromptOverrideConfiguration {
    static fromSteps(steps?: PromptStepConfiguration[]): PromptOverrideConfiguration;
    /**
     * Creates a PromptOverrideConfiguration with a custom Lambda parser function.
     * @param props Configuration including:
     *   - `parser`: Lambda function to use as custom parser
     *   - `steps`: prompt step configurations. At least one of the steps must make use of the custom parser.
     */
    static withCustomParser(props: CustomParserProps): PromptOverrideConfiguration;
    /**
     * The custom Lambda parser function to use.
     * The Lambda parser processes and interprets the raw foundation model output.
     * It receives an input event with:
     * - messageVersion: Version of message format (1.0)
     * - agent: Info about the agent (name, id, alias, version)
     * - invokeModelRawResponse: Raw model output to parse
     * - promptType: Type of prompt being parsed
     * - overrideType: Type of override (OUTPUT_PARSER)
     *
     * The Lambda must return a response that the agent uses for next actions.
     * @see https://docs.aws.amazon.com/bedrock/latest/userguide/lambda-parser.html
     */
    readonly parser?: IFunction;
    /**
     * The prompt configurations to override the prompt templates in the agent sequence.
     *
     * @default - No prompt configuration will be overridden.
     */
    readonly steps?: PromptStepConfigurationCustomParser[];
    /**
     * Create a new PromptOverrideConfiguration.
     *
     * @internal - This is marked as private so end users leverage it only through static methods
     */
    private constructor();
    /**
     * Format as CfnAgent.PromptOverrideConfigurationProperty
     *
     * @internal This is an internal core function and should not be called directly.
     */
    _render(): CfnAgent.PromptOverrideConfigurationProperty;
    private validateInferenceConfig;
    private validateSteps;
    private validateCustomParser;
}
